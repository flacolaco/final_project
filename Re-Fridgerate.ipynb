{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e36b461",
   "metadata": {},
   "source": [
    "# Re-Fridgerate: The Recipe Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99bacd",
   "metadata": {},
   "source": [
    "### [Webscraping](Webscraping)\n",
    "\n",
    "### [EDA, Cleaning & Wrangling](EDA)\n",
    "\n",
    "### [Sentiment Analysis](Sentiment)\n",
    "\n",
    "### [Minimum Viable Product (MVP)](Minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e3f0b",
   "metadata": {},
   "source": [
    "### 1. Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0dc88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing one url first\n",
    "\n",
    "url = \"https://www.allrecipes.com/recipe/22831/alfredo-sauce/\"\n",
    "\n",
    "# Downloading html with a request and getting response code\n",
    "\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the 'soup' with an hmtl parser\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129570a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the html code looks as expected\n",
    "\n",
    "soup.prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbdf139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the recipe title using inspect function in the website\n",
    "\n",
    "rawTitle = soup.find(class_=\"recipe-main-header\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ad333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From rawTitle we can get the actual title under \"headline\"\n",
    "\n",
    "parsedTitle = rawTitle.find(class_=\"headline\").get_text()\n",
    "parsedTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape categories: \n",
    "\n",
    "# Inspecting the website we see under \"breadcrumbs__title\" we see we have all categories and subcategories\n",
    "\n",
    "rawCategs = soup.find_all(class_=\"breadcrumbs__title\")\n",
    "rawCategs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55dca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty list and store all using a loop,\n",
    "# although we only care about items in the 3rd and 4th position from the list, which will be our Category and Subcategory\n",
    "\n",
    "categs = []\n",
    "for c in rawCategs:\n",
    "    categs.append(c.get_text())\n",
    "\n",
    "categs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b35038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we get our Category and Subcategories\n",
    "\n",
    "category = categs[2]\n",
    "subcategory = categs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda991dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping ingredients:\n",
    "\n",
    "# Inspecting the website we see all ingredients are nested individually under \"ingredients-item\"\n",
    "# we will find all of them first\n",
    "\n",
    "rawIngredients = soup.find_all(class_=\"ingredients-item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1cd4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will start by creating an empty list\n",
    "\n",
    "parsedIngredients = []\n",
    "\n",
    "# Then, we will loop through rawIngredients to find each \"ingredients-item-name\" and store it in a new variable \"ing\".\n",
    "# Finally, we will append \"ing\" in our empty list\n",
    "\n",
    "for i in rawIngredients:\n",
    "    ing = i.find(class_=\"ingredients-item-name\")\n",
    "    parsedIngredients.append(ing.get_text())\n",
    "    \n",
    "parsedIngredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86be3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping reviews:\n",
    "\n",
    "# Inspecting the website we see all reviews are stored under 'recipe-review-body--truncated', so we will use find_all\n",
    "    \n",
    "rawReviews = soup.find_all(class_='recipe-review-body--truncated')\n",
    "\n",
    "# Now we create and empty list, and loop through our previous results to append inside the list\n",
    "\n",
    "parsedReviews = []\n",
    "for r in rawReviews:\n",
    "    parsedReviews.append(r.get_text())\n",
    "    \n",
    "parsedReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the main dataframe\n",
    "\n",
    "df = pd.DataFrame(columns=['Title', 'Category', 'Subcategory', 'Ingredients', 'Reviews', 'URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that performs all the different scraping tasks we defined previously and stores it in our df.\n",
    "# We will later create a loop that will allow to apply this function across different urls\n",
    "\n",
    "def fetch_data(url, df):\n",
    "    \n",
    "    # Print message with url for trackability in case there is an error\n",
    "    print(\"Fetching \" + url)\n",
    "\n",
    "    # Getting the query\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # If response message different to 200, print error message and keep scraping.\n",
    "    # We were getting this error because not all pages where found when incrementing 'cur'\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error fetching page (Bummer!)\") \n",
    "        return df\n",
    "\n",
    "    # Create the soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find title\n",
    "    # Had to do some error handling since some pages had a different layout than the one I had originally scraped\n",
    "    try:\n",
    "        rawTitle = soup.find(class_=\"recipe-main-header\")\n",
    "        parsedTitle = rawTitle.find(class_=\"headline\").get_text()\n",
    "    \n",
    "    # Printing url with AttributeError to identify which urls had a different layout\n",
    "    except AttributeError:\n",
    "        print(\"AttributeError \" + url)\n",
    "        return df\n",
    "\n",
    "    # Find categories\n",
    "    rawCategs = soup.find_all(class_=\"breadcrumbs__title\")\n",
    "    categs = []\n",
    "    for c in rawCategs:\n",
    "        categs.append(c.get_text())\n",
    "    \n",
    "    # Created a loop that appends categs as blanks when these had a smaller length than 2 since some of the urls had no categories\n",
    "    categs = categs[2:4]\n",
    "    while len(categs) < 2:\n",
    "        categs.append(\"\")\n",
    "\n",
    "    # Find ingredients\n",
    "    rawIngredients = soup.find_all(class_=\"ingredients-item\")\n",
    "\n",
    "    # Parse ingredients\n",
    "    parsedIngredients = []\n",
    "    for i in rawIngredients:\n",
    "        ing = i.find(class_=\"ingredients-item-name\")\n",
    "        parsedIngredients.append(ing.get_text())\n",
    "\n",
    "    # Find reviews\n",
    "    rawReviews = soup.find_all(class_='recipe-review-body--truncated')\n",
    "\n",
    "    # Parse reviews\n",
    "    parsedReviews = []\n",
    "    for r in rawReviews:\n",
    "        parsedReviews.append(r.get_text())\n",
    "\n",
    "    # Creating a dictionary to append all scraped urls into the dataframe\n",
    "    row = {\n",
    "        'Title': parsedTitle, \n",
    "        'Category': categs[0],\n",
    "        'Subcategory': categs[1],\n",
    "        'Ingredients': parsedIngredients,\n",
    "        'Reviews': parsedReviews,\n",
    "        'URL': url,\n",
    "    }\n",
    "    df = df.append(row, ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166689cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a while loop that scrapes through multiple pages after noticing incrementing by one a specific section \n",
    "# of the url would take us to a new page with a new recipe\n",
    "\n",
    "# Scraping info: defining the baseUrl (which we will keep incrementing with the loop), the starting position and a total number of iterations 1500\n",
    "# With more time and processing power we could run more iterations\n",
    "\n",
    "baseUrl = \"https://www.allrecipes.com/recipe/\"\n",
    "startAt = 22121\n",
    "total = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa423b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While loop:\n",
    "# Defininig current as the previously defined starting position\n",
    "# While current is smaller than the starting position plus the total iterations, it will keep running\n",
    "\n",
    "cur = startAt\n",
    "while cur < startAt + total:\n",
    "    \n",
    "    # Construct url using our baseUrl and cur converted to string\n",
    "    url = baseUrl + str(cur)    \n",
    "    \n",
    "    # Calling the previously defined function\n",
    "    df = fetch_data(url, df)\n",
    "\n",
    "    # using sleep function with random floats from 0.0 to 1.0 seconds\n",
    "    wait_time = random.random()\n",
    "    sleep(wait_time)\n",
    "\n",
    "    # Increment count by one\n",
    "    cur += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a4920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final dataframe into a csv file\n",
    "\n",
    "df.to_csv('/Users/lucas/downloads/final_project.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f66e1a",
   "metadata": {},
   "source": [
    "### 2. EDA, Cleaning & Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc33b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aeb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv file and store into a dataframe\n",
    "\n",
    "df = pd.read_csv('final_project.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5597182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values, data types, total rows and columns\n",
    "\n",
    "df.info()\n",
    "\n",
    "# We have a few nulls in Category and Subcategory, but we will keep them for now\n",
    "# also, all columns are objects - we will need to change Ingredients to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322678fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the df's structure, we will deal with the necessary columns from left to right\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e2d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the whole df lower case\n",
    "\n",
    "df = df.applymap(lambda s:s.lower() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41715ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Unnamed: 0' column\n",
    "\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202aef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingredients dtype is an object, so we convert it to a list\n",
    "\n",
    "df['listIngredients'] = df['Ingredients'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd26863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ff17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will allow us to tokenize 'listIngredients'\n",
    "\n",
    "def tokenize (text):\n",
    "    tokens = word_tokenize(str(text))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423dae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function and store results in 'tokIngredients'\n",
    "\n",
    "df['tokIngredients'] = df['listIngredients'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only alpha values and storing them in 'alphaIngredients'\n",
    "\n",
    "df['alphaIngredients'] = df['tokIngredients'].apply(lambda x: [item for item in x if item.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of reserved words to remove from 'alphaIngredients'\n",
    "# Removing food \"state\", unit measures basic ingredients and others\n",
    "\n",
    "reserved_words = ['quarts', \n",
    "              'cup', \n",
    "              'cups', \n",
    "              'ounce', \n",
    "              'ounces',  \n",
    "              'pound', \n",
    "              'pounds', \n",
    "              'teaspoon',\n",
    "              'tablespoon',\n",
    "              'tablespoons',\n",
    "              'teaspoons',\n",
    "              'package',\n",
    "              'packages',\n",
    "              'all',\n",
    "              'purpose',\n",
    "              'lean',\n",
    "              'minced',\n",
    "              'ground',\n",
    "              'cubed',\n",
    "              'raw',\n",
    "              'fresh',\n",
    "              'frozen',\n",
    "              'extra' \n",
    "              'large',\n",
    "              'whole',\n",
    "              'small',\n",
    "              'lightly' \n",
    "              'battered',\n",
    "              'skinless',\n",
    "              'boneless',\n",
    "              'salt',\n",
    "              'pepper', \n",
    "              'vinegar', \n",
    "              'olive',\n",
    "              'vegetable',\n",
    "              'butter', \n",
    "              'water', \n",
    "              'onion', \n",
    "              'onions',\n",
    "              'garlic',\n",
    "              'potatoes', \n",
    "              'potato',\n",
    "              'flour',\n",
    "              'white',\n",
    "              'sugar',\n",
    "              'baking'\n",
    "              'soda',\n",
    "              'powder',\n",
    "              'peeled',\n",
    "              'chopped',\n",
    "              'sliced',\n",
    "              'can',\n",
    "              'dried',\n",
    "              'ketchup',\n",
    "              'sauce'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36afead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing reserved words from 'alphaIngredients', storing result in 'cleanIngredients'\n",
    "\n",
    "df['cleanIngredients'] = df['alphaIngredients'].apply(lambda x: [item for item in x if item not in reserved_words])\n",
    "\n",
    "# Quick check\n",
    "\n",
    "df['cleanIngredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03dae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking an example of a review, we see one of them is duplicated\n",
    "\n",
    "df['Reviews'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfd036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate values from 'Reviews'\n",
    "\n",
    "df['Reviews'] = df['Reviews'].apply(lambda x: ','.join(pd.unique(x.split(','))))\n",
    "\n",
    "# Quick check\n",
    "\n",
    "df['Reviews'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of records per Category. This could influence the scope of our MVP\n",
    "\n",
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be59be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual check\n",
    "\n",
    "df['Category'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489de9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of records per Subcategory\n",
    "\n",
    "df['Subcategory'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f67220",
   "metadata": {},
   "source": [
    "### 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbd4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample check on the review column, index position 3 \n",
    "\n",
    "text = df.iloc[3]['Reviews']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f753791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing text variable into new variable 'testimonial'\n",
    "\n",
    "testimonial = TextBlob(text)\n",
    "\n",
    "# Running sentiment analysis on the selected record\n",
    "\n",
    "testimonial.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TextBlob to break down the review into sentences to facilitate reading.\n",
    "# Perform a \"sense check\" of the polarity obtained above\n",
    "\n",
    "testimonial.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation from 'Reviews' using RegEx and storing in a new column\n",
    "\n",
    "df['cleanReviews'] = df['Reviews'].str.replace('[^\\w\\s]','')\n",
    "df['cleanReviews'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc0eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Text and stopwords from NLTK libraries\n",
    "# Printing stop_words to double check we remove them properly later on\n",
    "\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing 'cleanReviews' to facilitate the removal of stopwords. Storing in column 'tokReviews'\n",
    "\n",
    "tokenized = word_tokenize(text)\n",
    "df['tokReviews'] = df['cleanReviews'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Quick check\n",
    "\n",
    "df['tokReviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85631f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words from 'tokReviews', storing in 'sentReviews'\n",
    "\n",
    "df['sentReviews'] = df['tokReviews'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# Quick check\n",
    "\n",
    "df['sentReviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column 'Sentiment' with 'sentReviews' sentiment scores\n",
    "\n",
    "df['Sentiment'] = df['Reviews'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d1255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at descriptive statistics for 'Sentiment'\n",
    "\n",
    "df['Sentiment'].describe()\n",
    "\n",
    "# looking at the mean and different quartiles we see reviews are almost always positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average rating per category stored in a new df 'avgCateg'\n",
    "\n",
    "avgCateg = df.groupby([\"Category\"])[\"Sentiment\"].mean().reset_index()\n",
    "avgCateg.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising previous result. It seems like reviewers are more positive with drinks and fruits & veggies.\n",
    "# We would need to compare this with the average number of reviews for each category in a future iteration\n",
    "\n",
    "avgCateg = avgCateg.sort_values(by='Sentiment')\n",
    "avgCateg.plot(kind=\"barh\", y='Sentiment', x='Category');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3c8f80",
   "metadata": {},
   "source": [
    "### 4. Minimum Viable Product (MVP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57844939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipe_recommender():\n",
    "    \n",
    "    # Print welcome message and instructions\n",
    "    \n",
    "    print('Welcome to Re-Fridgerate!')\n",
    "    print(\"Let me know what's in your fridge and I will recommend a few of our best reviewed recipes!\")\n",
    "    print('I will now ask you to enter your food ingredients one by one')\n",
    "    \n",
    "    # Store user input in a list of length 3\n",
    "    \n",
    "    userInput = []\n",
    "    for i in range(3):\n",
    "        userInput.append(input('Ingredient: ').lower())\n",
    "    \n",
    "    # Create variable 'suggestion' that stores a subset of our dataframe where any cleanIngredient is in userInput\n",
    "    \n",
    "    suggestion = df[df['cleanIngredients'].apply(lambda x: any(item in userInput for item in x))] #.sample(5)\n",
    "    \n",
    "    # Select only Title, Category and URL and store in 'suggestion'\n",
    "    \n",
    "    suggestion = suggestion[['Title', 'Category', 'URL']]\n",
    "    print('Below our suggestions... Bon Apetit!')\n",
    "    return suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65951b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_recommender()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
